---
title: "P8105 Hw6"
output: github_document
---

```{r setup}
library(tidyverse)
library(readr)
library(mgcv)
library(modelr)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  fig.width = 8,
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Problem 1

Firstly load and clean the data

```{r}
birthweight = read_csv("./data/birthweight.csv")

birthweight = 
  birthweight %>% 
  mutate(
    babysex = factor(babysex),
    frace = factor(frace),
    malform = factor(malform),
    mrace = factor(mrace),
    babysex = fct_recode(babysex, "male" = "1", "female" = "2"),
    frace = fct_recode(frace, "White" = "1", "Black" = "2", "Asian" = "3", 
                       "Puerto Rican" = "4", "Other" = "8", "Unkonwn" = "9"),
    malform = fct_recode(malform, "absent" = "0", "present" = "1"),
    mrace = fct_recode(mrace, "White" = "1", "Black" = "2", "Asian" = "3",
                       "Puerto Rican" = "4", "Other" = "8")
  )
```

Make a regression model

Since there are lots of variables, would try to use backward elimination

```{r results='hide'}
# fit the regression, full model
mult.fit = lm(bwt ~ ., data = birthweight)
mult.fit %>% 
  broom::tidy()
# backward elimination
back_fit = step(mult.fit, direction = 'backward')
```

```{r}
back_fit %>% 
  broom::tidy()
```

So these are the prediction variables that my model contains.

Now get the plot of model residuals against fitted values

```{r}
birthweight_myfit = 
  birthweight %>% 
  modelr::add_predictions(back_fit, var = "fitted") %>% 
  modelr::add_residuals(back_fit, var = "residual")

birthweight_myfit %>% 
  ggplot(aes(x = fitted, y = residual)) + geom_point(alpha = .3) + 
  labs(
    title = "Model Residuals Against Fitted Values",
    x = "Fitted Values",
    y = "Residuals"
  )
```

Now compare the models, split the data to train and test, and calculate the RMSE.

```{r}
cv_df = 
  crossv_mc(birthweight, 100)

cv_df = 
  cv_df %>% 
  mutate(
    my_mod  = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + 
                               fincome + gaweeks + mheight + mrace + parity + 
                               ppwt + smoken, data = .x)),
    mod_1  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    mod_2  = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead * blength + 
                              bhead * babysex + blength * babysex + 
                              bhead * blength * babysex, data = .x))) %>% 
  mutate(
    rmse_my_mod = map2_dbl(my_mod, test, ~rmse(model = .x, data = .y)),
    rmse_mod_1 = map2_dbl(mod_1, test, ~rmse(model = .x, data = .y)),
    rmse_mod_2 = map2_dbl(mod_2, test, ~rmse(model = .x, data = .y)))
```

Draw the violin plot of RMSE for each model.

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, fill = model)) + geom_violin(alpha = .4)
```

So, my model tends to have the best prediction compared with the other 2 models.


# Problem 2

Get the data

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Visualize the tmax and tmin

```{r}
weather_df %>% 
  ggplot(aes(x = tmin, y = tmax)) + geom_point()
```

Try to do the linear regression on the whole data firstly and have a check of r square and log(beta0 * beta1).

```{r}
fit = lm(tmax ~ tmin, data = weather_df)

# The model r.square
fit %>% 
  broom::glance() %>% 
  .[[1]]

# The model log(beta0 * beta1)
coeff = fit %>% 
  broom::tidy()
log(coeff[[2]][1] * coeff[[2]][2])
```

Now Start Bootstrapping:

```{r}
# bootstrapping
bootstrap = 
  weather_df %>% 
  bootstrap(n = 5000) %>% 
   mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    glance = map(models, broom::glance),
    results = map(models, broom::tidy))

# write a function to calculate the log(b0 * b1) of the model
# the variable it takes is the result of broom::tidy()
logg = function(tidy){
  log(tidy[[2]][1] * tidy[[2]][2])
}

# extract
boot_result = 
  bootstrap %>% 
  select(-strap, -models) %>% 
  unnest(glance) %>% 
  select(.id, r.squared, results) %>% 
  mutate(log_var = map(.x = results, ~logg(.x))) %>% 
  select(-results) %>% 
  unnest(log_var)
```

### Distribution of r.squared

```{r}
boot_result %>% 
  ggplot(aes(x = r.squared)) + geom_density() + labs(
    title = "Distribution of r.squared"
  )
```

We could notice that the r.squared is nearly normal, and it has only a little heavy tail.

### Distribution of log(b0 * b1)

```{r}
boot_result %>% 
  ggplot(aes(x = log_var)) + geom_density() + labs(
    title = "Distribution of log(beta_0 * beta_1)",
    x = "log(beta_0 * beta_1)"
  )
```

Here we could see that the log variable is more likely to be normal distributed.
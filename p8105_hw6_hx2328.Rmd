---
title: "P8105 Hw6"
output: github_document
---

```{r setup}
library(tidyverse)
library(readr)
library(mgcv)
library(modelr)

knitr::opts_chunk$set(
  echo = TRUE,
  warning = FALSE,
  fig.width = 8,
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

# Problem 1

Firstly load and clean the data

```{r}
birthweight = read_csv("./data/birthweight.csv")

birthweight = 
  birthweight %>% 
  mutate(
    babysex = factor(babysex),
    frace = factor(frace),
    malform = factor(malform),
    mrace = factor(mrace),
    babysex = fct_recode(babysex, "male" = "1", "female" = "2"),
    frace = fct_recode(frace, "White" = "1", "Black" = "2", "Asian" = "3", 
                       "Puerto Rican" = "4", "Other" = "8", "Unkonwn" = "9"),
    malform = fct_recode(malform, "absent" = "0", "present" = "1"),
    mrace = fct_recode(mrace, "White" = "1", "Black" = "2", "Asian" = "3",
                       "Puerto Rican" = "4", "Other" = "8")
  )
```

Make a regression model

Since there are lots of variables, would try to use backward elimination

```{r results='hide'}
# fit the regression, full model
mult.fit = lm(bwt ~ ., data = birthweight)
mult.fit %>% 
  broom::tidy()
# backward elimination
back_fit = step(mult.fit, direction = 'backward')
```

```{r}
back_fit %>% 
  broom::tidy()
```

So these are the prediction variables that my model contains.

Now get the plot of model residuals against fitted values

```{r}
birthweight_myfit = 
  birthweight %>% 
  modelr::add_predictions(back_fit, var = "fitted") %>% 
  modelr::add_residuals(back_fit, var = "residual")

birthweight_myfit %>% 
  ggplot(aes(x = fitted, y = residual)) + geom_point(alpha = .3) + 
  labs(
    title = "Model Residuals Against Fitted Values",
    x = "Fitted Values",
    y = "Residuals"
  )
```

Now compare the models, split the data to train and test, and calculate the RMSE.

```{r}
cv_df = 
  crossv_mc(birthweight, 100)

cv_df = 
  cv_df %>% 
  mutate(
    my_mod  = map(train, ~lm(bwt ~ babysex + bhead + blength + delwt + 
                               fincome + gaweeks + mheight + mrace + parity + 
                               ppwt + smoken, data = .x)),
    mod_1  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    mod_2  = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead * blength + 
                              bhead * babysex + blength * babysex + 
                              bhead * blength * babysex, data = .x))) %>% 
  mutate(
    rmse_my_mod = map2_dbl(my_mod, test, ~rmse(model = .x, data = .y)),
    rmse_mod_1 = map2_dbl(mod_1, test, ~rmse(model = .x, data = .y)),
    rmse_mod_2 = map2_dbl(mod_2, test, ~rmse(model = .x, data = .y)))
```

Draw the violin plot of RMSE for each model.

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, fill = model)) + geom_violin(alpha = .4)
```

So, my model tends to have the best prediction compared with the other 2 models.


# Problem 2



